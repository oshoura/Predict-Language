Taking Faster and Smarter to New Physical Frontiers
The value of computers is often measured with terms like gigahertz, petaflop and exabyte the speed, scale and efficiency with which they perform computations.
But so what? The intrinsic worth of computation ought to be a matter of the types of information being computed upon, and of when and where these computations occur. When you are lost, for example, a phone that will compute your route home is worth far more to you than an inaccessible desktop PC, no matter how fancy and powerful.
So a better way to think about the future of computing might be to ask when and where we could improve our ability to compute upon information that we greatly care about.
Consider a simple "computer" that counts to just 256, using eight bits of data storage. Could such a computer have any significant value?
Well, what if such computers could be installed inside every cell of your body? What if these computers were used to keep track of how many times each of your cells divided, forming the basis of systems that could track and control aging, development and cancer? If too many divisions are detected, programmed cell death could be set off before a tumor had a chance to form.
Such computers would have incredible value for basic research and medical biotechnology. But note that these computers would derive all their value from being able to connect with and compute inside living cells, a place that no silicon-based computer can now operate.
So would it be possible to build simple computers that operate inside living cells or other new places? Yes though in the case of biology, such computers would most likely need to be engineered from biological molecules.
Fortunately, sustained basic research since the early 1980s has taught us how natural biological systems might be adapted to store information and compute upon it. Now, researchers are starting to systematically apply this knowledge to develop scalable living data storage systems and simple computational interfaces.
For example, digital information can be stored inside cells by flipping DNA sequences back and forth or by controlling protein levels and locations. In more detail, a DNA sequence that can be switched between two possible orientations can be used to represent and store "0" in one orientation and "1" in the other, which taken together represent one bit.
Other research is exploring how RNA, for example, can be engineered to control the reading and writing of information to synthetic biological bits. But we are still a long way from achieving a reliable genetically encoded eight-bit counter.
When and where else might we find new places to compute upon information? Obviously, improved access to information and computation in many human and cultural systems will be important, from seemingly mundane advances in conference room scheduling or building ventilation to improved medical and political decision-making systems.
But most such futures are foreseeable; they will be based on traditional computing architectures and engineered silicon, and they are likely to be limited by an array of human factors: for example, who controls the room-scheduling system? Who gets access, and how can access be better scaled?
I dream of much broader achievements. For example, suppose we could partner with microbes and plants to record events, natural or otherwise, and convert this information into easily observed signals. That would greatly expand our ability to monitor the environment. And I imagine foods that tell us whether they are organic just by looking at them.
So the future of computing need not only be a question of putting people and things together with ubiquitous silicon computers. The future will be much richer if we can imagine new modes of computing in new places and with new materials and then find ways to bring those new modes to life.
Quantum Computing Promises New Insights, Not Just Supermachines
When people hear that I work on quantum computing one of the most radical proposals for the future of computation their first question is usually, "So when can I expect a working quantum computer on my desk?" Often they bring up breathless news reports about commercial quantum computers right around the corner. After I explain the strained relationship between those reports and reality, they ask: "Then when? In 10 years? Twenty?"
Unfortunately, this is sort of like asking Charles Babbage, who drew up the first blueprints for a general-purpose computer in the 1830s, whether his contraption would be hitting store shelves by the 1840s or the 1850s. Could Babbage have foreseen the specific technologies   the vacuum tube and transistor   that would make his vision a reality more than a century later? Today's quantum computing researchers are in a similar bind. They have a compelling blueprint for a new type of computer, one that could, in seconds, solve certain problems that would probably take eons for today's fastest supercomputers. But some of the required construction materials don't yet exist.
So you might think quantum computers are something real scientists   as opposed to science-fiction buffs   won't need to worry about for a long time. But I'd urge a different view. Quantum computing really is one of the most exciting things happening in science right now. Just not for the reasons you usually hear.
First, though, what is a quantum computer? Walk into a quantum computing lab, and you won't see much: maybe a fist-size "trap" where ions (often cadmium or calcium) are suspended in a magnetic field, a laser for moving the ions around, a computer screen with a row of flickering white blobs representing the ions' approximate locations. The real action, one might say, is happening in a different realm entirely: in the alien mathematics that governs what the ions are doing.
Struggling to shoehorn that mathematics into newspaper-friendly metaphors, most popular writers describe a quantum computer as a magic machine that could process every possible answer in parallel, rather than trying them one at a time. Supposedly, it could do that because, unlike today's computers that manipulate bits, a quantum computer would manipulate quantum bits, or qubits, which can be 0 and 1 simultaneously.
But that's a crude way to visualize what a quantum computer does, and misses the most important part of the story. When you measure a quantum computer's output, you see just a single, random answer   not a listing of all possible answers. Of course, if you had merely wanted a random answer, you could have picked one yourself, with much less trouble.
Thus, the sole reason to prefer a quantum computer is that the subatomic world obeys different laws of probability than the ones we are used to. In everyday life, it would be silly to speak of a "minus 30 percent chance of rain tomorrow," much less a "square root of minus 1 percent chance." However, quantum mechanics is based on numbers called amplitudes, which are closely related to probabilities but can also be negative (in fact, they are complex numbers). Crucially, if an event (say, a photon hitting a screen) can happen one way with positive amplitude, and a different way with negative amplitude, then the two amplitudes can "interfere destructively" and cancel each other out, so that the event never happens at all. The goal in quantum computing is to choreograph a computation so that the amplitudes leading to wrong answers cancel each other out, while the amplitudes leading to right answers reinforce.
Contrary to a widespread misconception, it's for only a few specialized types of problem that researchers know how exploit that trick to obtain dramatic speed increases over today's computers. To date, the two main examples are simulating the behavior of atoms and molecules, and breaking certain cryptographic codes   including, by unlucky coincidence, most of the codes used in modern electronic commerce. Of course, both problems can be tackled with conventional computers, too   but, though no one has proved it, it's plausibly conjectured that any conventional algorithm would need a huge amount of time. While code-breaking understandably grabs the headlines, it's the more humdrum application of quantum computers   simulating quantum physics and chemistry   that has the potential to revolutionize fields from nanotechnology to drug design.
Scott Aaronson is an associate professor of electrical engineering and computer science at M.I.T.

Page 2 of 2
Unfortunately, while small quantum computations have already been demonstrated in the lab, they typically fall apart after only a few dozen operations. That's why one of the most-celebrated quantum computations to date has been to factor 15 into 3 times 5   with high statistical confidence! The problem is decoherence: basically, stray interactions that intrude prematurely on the computer's fragile quantum state, "collapsing" it like a souffle. In theory, it ought to be possible to reduce decoherence to a level where error-correction techniques could render its remaining effects insignificant. But experimentalists seem nowhere near that critical level yet.
And yet, even though useful quantum computers might still be decades away, many of their payoffs are already arriving. For example, the mere possibility of quantum computers has all but overthrown a conception of the universe that scientists like Stephen Wolfram have championed. That conception holds that, as in the "Matrix" movies, the universe itself is basically a giant computer, twiddling an array of 1's and 0's in essentially the same way any desktop PC does.
Quantum computing has challenged that vision by showing that if "the universe is a computer," then even at a hard-nosed theoretical level, it's a vastly more powerful kind of computer than any yet constructed by humankind. Indeed, the only ways to evade that conclusion seem even crazier than quantum computing itself: One would have to overturn quantum mechanics, or else find a fast way to simulate quantum mechanics using today's computers.
Setting aside these cosmic concerns, there are more practical spinoffs from research in quantum computing. Techniques invented to understand quantum algorithms have repeatedly proved useful for understanding conventional algorithms   and there are now cryptographic codes, purely for conventional computers, for which the main evidence of their security comes from arguments based on quantum computing. Quantum computing ideas have also influenced chemistry and physics: Several research groups have used quantum-computing analogies to explain the remarkable light-harvesting efficiency of photosynthetic molecules in plants, and to suggest how solar panels might be designed with similar efficiencies.
But the biggest payoff so far may have been an improvement in the way quantum mechanics itself is taught and understood. Since its beginnings in the 1920s, quantum mechanics has been considered the prototype of an abstruse, complicated theory: something beyond the grasp of all but a few physicists. Today, though, I and others regularly explain its underlying logic to students by focusing on the simplest imaginable system to which that logic applies: the qubits that make up a quantum computer.
Like fusion power, practical quantum computers are a tantalizing possibility that the 21st century may or may not bring   depending on the jagged course not only of science and technology, but of politics and economics. By contrast, as a scientific endeavor that combines many of the deepest questions of physics and computer science, there's no need to wait for quantum computing: It's already here.
Scott Aaronson is an associate professor of electrical engineering and computer science at M.I.T.
An Evolution Toward a Programmable Universe
Over the next 10 years, the physical world will become ever more overlaid with devices for sending and receiving information.
Already billions of processors are embedded in our smartphones, cars, appliances and buildings and the environment. These sensors can send out streams of data about their surroundings, and more and more it is anonymously transmitted to remote data centers the "clouds" of Google, Amazon, Microsoft, Yahoo and Apple.
From these vast clouds, the companies can power apps that are "spatially aware." For instance, Google Maps now draws on data in the cloud to sample the location and movement of cellphones in cars, producing a real-time picture of traffic congestion.
Smart electric grids are measuring our homes' use of power; active people are tracking their heart rates; and hundreds of millions of us are uploading geo-tagged data to Flickr, Yelp, Facebook and Google Plus. As we look 10 years ahead, the fastest supercomputer (the "exascale" machine) will be composed of one billion processors, and the clouds will most likely grow to this scale as well, creating a distributed planetary computer of enormous power.
Such computational power, co-located with the gigantic storage that holds the data from all the incoming data streams, will enable faster-than-real-time simulations of many aspects of our physical world. As Mike Liebhold and his colleagues at the Institute for the Future have discussed, computing will have evolved from merely sensing local information to analyzing it to being able to control it. In this evolution, the world gradually becomes programmable.
At the California Institute for Telecommunications and Information Technology, we are using this vision to better understand the coming digital transformation of health, energy, environment and culture. We are experimenting with sensors to monitor electricity use in homes, buildings and data centers; the data can then be analyzed and used to control lighting, heating, cooling, appliances and computers to make them more energy-efficient.
It is logical that the analysis of traffic data, coupled with in-car radar and autopilot electronics, will enable software control of large numbers of robot-driven electric cars. Since buildings and transportation are major sources of greenhouse gas emissions, the sensor-aware planetary computer can be a crucial factor in reducing our carbon footprints.
The same principle applies to our bodies. I wear sensors to measure my steps, caloric burn and sleep patterns, while heart patients can wear sensors that wirelessly notify their doctors of life-threatening conditions. People will soon be able to have their genetic code and medical imaging stored in the cloud, along with charts of vital signs and detailed nutritional analysis of everything they consume.
Using this data, the planetary computer will be able to build a computational model of your body and compare your sensor stream with millions of others. Besides providing early detection of internal changes that could lead to disease, cloud-powered voice-recognition wellness coaches could provide continual personalized support on lifestyle choices, potentially staving off disease and making health care affordable for everyone.
Finally, in culture, the fine-grain streaming provided by Twitter, Facebook and Google Plus enables us to map out phenomena using "human sensors."
For instance, a vast power failure occurred in Southern California in September; within minutes we could tell from the locations of Twitter messages saying "my power just went out" that it was widespread, long before the official announcement. Similarly, Twitter feeds from large geographic areas have been analyzed to create dynamic "social mood" or "political anger" maps, like the Google traffic maps constructed from GPS feeds.
Conceivably, the coupling of the sensor and human streams with planetary computing power will make it possible to create "social forecasts." For good or evil, it seems inevitable that individuals, corporations, political leaders and intelligence agencies will come to use planetary computer models of social behavior to inject content into the global attention stream at just the right moment, hoping to steer the social dynamics to a desired outcome.
With the continuing exponential increase in the power of the planetary computer, one has to wonder whether we stand at the beginning of what Isaac Asimov's "Foundation" series, more than 60 years ago, called "psychohistory." His visionary genius Hari Seldon believed that statistical forecasting of human society's actions would be possible with data from enough people throughout the galaxy.
In the next several decades, we will have a glimpse of whether something similar can emerge on planet Earth.

In an Open-Source Society, Innovating by the Seat of Our Pants

The Internet isn't really a technology. It's a belief system, a philosophy about the effectiveness of decentralized, bottom-up innovation. And it's a philosophy that has begun to change how we think about creativity itself.

Almost 20 years ago, I installed on my computer a tiny piece of software called MacPPP, which connected the programs running on it to the Internet. The program immediately transformed my computer from a fancy telex machine to a device running a very early version of the graphical Web.

I was working in entertainment at the time, and I remember thinking that this connection was going to change everything. I left to join the first commercial Internet service provider in Japan, PSINet Japan, as its first chief executive. Our first serious challenge, oddly enough, was a battle over an obscure information-sharing computer protocol called X.25. Most of us laboring to build the new Internet preferred the less regulated and simpler Internet Protocol.

Until then, large intergovernmental agencies had always gathered experts to work on the technical standards that would become the DNA of the telecommunications industry, the standards to which all companies would have to build their networks and products. These researchers had produced X.25, a complex and extremely well-considered standard that seemed to anticipate every possible problem and application.

The Internet, on the other hand, was designed and deployed by small groups of researchers following the credo of one of its chief architects, David Clark: "rough consensus and running code." Its early standards   uncomplicated, consensual   were stewarded by small organizations that resisted permission or authority. And they won: The Internet Protocol on which every connected device relies was a triumph of distributed innovation over centralized expertise.

The ethos of the Internet is that everyone should have the freedom to connect, to innovate, to program, without asking permission. No one can know the whole of the network, and by design it cannot be centrally controlled. This network was intended to be decentralized, its assets widely distributed. Today most innovation springs from small groups at its "edges."

This technical strategy has led to the creation of a gigantic network of far-flung innovators who develop standards with one another and share the products of their work in the form of free and open-source software. The architecture of the Internet and its abundance of free software and components has driven down the cost of manufacturing, distribution and collaboration   of innovation. It used to cost millions of dollars to start a software company. Today, for little or no money, entrepreneurs are able to develop and release a "minimum viable product" and test it with real users on the Internet before they have to raise any money from investors. In their earliest iterations, Facebook, Yahoo and Google were running in dorm rooms and labs before the founders had left college or had raised outside money.

In fact, it is now usually cheaper to just try something than to sit around and try to figure out whether to try something. The product map is now often more complex and more expensive to create than trying to figure it out as you go. The compass has replaced the map, and "rough consensus and running code" has become the fundamental philosophy for the so-called lean start-up movement.

Innovators are able to prototype a new product with 3-D printers and cheap laser cutters for nearly nothing. Even complex products can be manufactured with help from supply chain companies that are making their systems available online to anybody. Today we are seeing the emergence of a community of hardware hackers and designers very reminiscent of the developers who wrote the original open standards of the Internet. An explosion of grass-roots innovation in hardware is coming   freely designed and freely shared   as it did in software.

What has been a wildly successful model for consumer Internet start-ups in Silicon Valley turns out to be an extremely good model for learning in a wide variety of fields and disciplines. The students at M.I.T.'s Media Lab experiment, create and iterate; they produce demos and prototypes, and share and collaborate with the rest of the world through the Internet and a distributed network of connections and relationships.

I don't think education is about centralized instruction anymore; rather, it is the process establishing oneself as a node in a broad network of distributed creativity.

Neoteny, one of my faComputer Scientists May Have What It Takes to Help Cure Cancer
The war against cancer is increasingly moving into cyberspace. Computer scientists may have the best skills to fight cancer in the next decade - and they should be signing up in droves.
One reason to enlist: Cancer is so pervasive. In his Pulitzer Prize-winning book, "The Emperor of All Maladies," the oncologist Siddhartha Mukherjee writes that cancer is a disease of frightening fractions: One-fourth of deaths in the United States are caused by cancer; one-third of women will face cancer in their lifetimes; and so will half of men.
As he wrote, "The question is not if we will get this immortal disease, but when."
Dr. Mukherjee noted that surprisingly recently, researchers discovered that cancer is a genetic disease, caused primarily by mutations in our DNA. As well as providing the molecular drivers of cancer, changes to the DNA also cause the diversity within a cancer tumor that makes it so hard to eradicate completely.
The hope is that by sequencing the genome of a cancer tumor, doctors will soon be able to prescribe a personalized, targeted therapy to stop a cancer's growth or to cure it.
According to Walter Isaacson's new biography "Steve Jobs," a team of medical researchers sequenced the Apple executive's pancreatic cancer tumor and used that information to decide which drug therapies to use. Since Mr. Jobs's cancer had already spread, this effort was even more challenging. Each sequencing cost $100,000.
Fortunately for the rest of us, the cost of turning pieces of DNA into digital information has improved: The costs dropped a hundredfold in the last three years. The tipping point before widespread use is believed to be $1,000 per individual genome, which is a reason for the major investment in reducing its cost. Given such dramatic improvement, we could soon afford to sequence the genomes of the millions of cancer patients, which only billionaires could afford a few years ago.
How can computer scientists help?
First, as recently reported in this newspaper, the cost of millions of short reads of one cell by a gene sequencing machine is dwarfed by the data processing costs to turn them into a single usable three-billion-base-pair digital representation of a genome. To make personalized medicine affordable for everyone, we need to drive down the information processing costs.
Second, we need to collect cancer genomes in a repository and make them available to scientists and health professionals. The computer scientist David Haussler of the University of California, Santa Cruz, for example, is creating one. Plans are that this five-petabyte (5,000,000,000,000,000 bytes) store will house more than 20,000 genomes.
Third, finding a personalized, targeted therapy for each tumor among myriad possible combinations of drugs is like finding a very small needle in a very large haystack. Researchers are exploring the engagement of people when traditional hardware and software are not up to the task.
An inspirational example is the Foldit game; developed by the computer scientist Zoran Popovic at the University of Washington; that recently attracted thousands of volunteers to uncover the structure of an enzyme important to H.I.V. research.
Cancer tumor genomics is just one example of the Big Data challenge in computer science. Big Data is unstructured, uncurated and inconsistent, and housing it often requires a thousand-fold increase in size over traditional databases. It is not pristine data that can be neatly stored in rows and columns. YouTube alone holds nearly one exabyte of videos, which is one trillion megabytes, or 1,000,000,000,000,000,000 bytes.
The Big Data research challenge is to develop technology that can obtain timely and cost-effective answers to Big Data questions. A Berkeley team of eight faculty members and 40 Ph.D. students is rising to that challenge via three initiatives: inventing algorithms based on statistical machine learning; harnessing many machines in the cloud; and developing crowd-sourcing techniques to get people to help answer questions that prove too hard for our algorithms and machines.
Algorithms, machines and people gave our new lab its name: the AMP Lab.
AMP technology could help the war on cancer. It needs new algorithms to find those needles in haystacks. To process genome data faster and more cheaply, the war needs new infrastructure to use many machines in the cloud simultaneously. And it needs to be able to engage the wisdom of the crowd when the problems of cancer genome discovery and diagnosis are beyond our algorithms and machines.
It may have been true once that expertise in computer science was needed only by computer scientists. But Big Data has shown us that's no longer the case. It is entirely possible that we have the skill sets needed now to fight cancer and to advance sciences in myriad other ways.
The night after we made that argument, I awoke in the middle of the night with this question etched into my mind: Given that millions of people do have and will get cancer, if there is a chance that computer scientists may have the best skill set to fight cancer today, as moral people aren't we obligated to try?
David Patterson is a professor in computer science at the University of California, Berkeley.
vorite words, means the retention of childlike attributes in adulthood: idealism, experimentation and wonder. In this new world, not only must we behave more like children, we also must teach the next generation to retain those attributes that will allow them to be world-changing, innovative adults who will help us reinvent the future.

Joichi Ito is the director of the M.I.T. Media Lab.

Larry Smarr is the founding director of Calit2.
